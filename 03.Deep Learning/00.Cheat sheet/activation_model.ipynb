{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions  \n",
    "In the forward pass, we need an activation functions to activativate perceptrons in the next layer.  \n",
    "This activation function will forward weights of previous layer to next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Most popular Activation functions are`  \n",
    "1. Step function\n",
    "2. Sigmoid\n",
    "3. TanH\n",
    "4. Relu\n",
    "5. Leaky ReLU\n",
    "6. Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Step Function  \n",
    "Not Used in the practice.\n",
    "2. Sigmoid function  \n",
    "Typically, used in the last layer of a `binary Classification Problem`.\n",
    "3. TanH function  \n",
    "Good choice in the `Hidden Layers`.  \n",
    "4. ReLU function  \n",
    "If you dont know what to use, Just use a `ReLU function for Hideen Layers`.  \n",
    "5. Leaky ReLU function.  \n",
    "Updated/Improved Version of ReLU function.  \n",
    "Some times the loss doesnot decrease due to Vanishing Gradient Problem of ReLU during Training.  \n",
    "At that time use Leaky Relu. `Leaky ReLU tries to solve Vanishing Gradient Problem of ReLU`.  \n",
    "6. Softmax.  \n",
    "Softmax tries to convert Logits/Scores of output layer into Probabilities.  \n",
    "It is very good in `Last layer of Multi-class Classification.`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "import torch as T\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (input_layer): Linear(in_features=784, out_features=600, bias=True)\n",
       "  (hidden1): Linear(in_features=600, out_features=400, bias=True)\n",
       "  (hidden2): Linear(in_features=400, out_features=200, bias=True)\n",
       "  (hidden3): Linear(in_features=200, out_features=100, bias=True)\n",
       "  (output_layer): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    # Deining layers\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(784,600)\n",
    "        self.hidden1 = nn.Linear(600,400)\n",
    "        self.hidden2 = nn.Linear(400,200)\n",
    "        self.hidden3 = nn.Linear(200,100)\n",
    "        self.output_layer = nn.Linear(100,10)\n",
    "\n",
    "    # forward pass through the network\n",
    "    def forward(self, x):\n",
    "        first_layer = self.input_layer(x)\n",
    "        act1 = F.relu(first_layer)\n",
    "        second_layer = self.hidden1(act1)\n",
    "        act2 = F.relu(second_layer)\n",
    "        third_layer = self.hidden2(act2)\n",
    "        act3 = F.relu(third_layer)\n",
    "        fourth_layer = self.hidden3(act3)\n",
    "        act4 = F.relu(fourth_layer)\n",
    "        out_layer = self.output_layer(act4)\n",
    "        #x = F.softmax(out_layer, dim=1)\n",
    "        return out_layer\n",
    "    \n",
    "model = Classifier()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (input_layer): Linear(in_features=784, out_features=600, bias=True)\n",
       "  (hidden1): Linear(in_features=600, out_features=400, bias=True)\n",
       "  (hidden2): Linear(in_features=400, out_features=200, bias=True)\n",
       "  (hidden3): Linear(in_features=200, out_features=100, bias=True)\n",
       "  (output_layer): Linear(in_features=100, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    # Deining layers\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(784,600)\n",
    "        self.hidden1 = nn.Linear(600,400)\n",
    "        self.hidden2 = nn.Linear(400,200)\n",
    "        self.hidden3 = nn.Linear(200,100)\n",
    "        self.output_layer = nn.Linear(100,10)\n",
    "    # forward pass through the network\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.input_layer(x))\n",
    "        x = F.relu(self.hidden1(x))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "        x = F.relu(self.hidden3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "model1 = Classifier()\n",
    "model1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use nn.squential to define our neural network model.  \n",
    "refer to MNIST exercise."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0d2eac5464fc2e87cb93eb40af5f568dfd8212119c3ce5d2e079388acbe40c19"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dl22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
