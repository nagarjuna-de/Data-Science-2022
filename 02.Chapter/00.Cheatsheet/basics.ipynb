{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Scaler\n",
    "Standard Scaler is used to normalize the data. It shifts all the values in the column by its **column mean** and divided by the **standard deviation**.  \n",
    "z = (x - u) / s\n",
    "\n",
    "[sklearn standard scaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)  \n",
    "**from sklearn.preprocessing import StandardScaler**  \n",
    "scaler = StandardScaler  \n",
    "We need to fit our data into the standard scaler.then only standard scaler performs mean or std  \n",
    "**scaler.fit(a)**  \n",
    "**scaler.transform(a)**  \n",
    "In one step then  \n",
    "**scaler.fit_transform(a)**  \n",
    "To get back the actual data after the transform.\n",
    "**scaler.inverse_transform(scaler.fit_transform(a))**\n",
    "### Raise value Error.  \n",
    "The array needs to be in 2-dimension, otherwise gives Raise value error of *Expected 2D Array and got 1D array Instead.*  \n",
    "#### Resolving error\n",
    "Reshape the array. a.reshape(-1,1)  \n",
    "**Note - Dont use StandardScaler before splitting(test-train-split) the data.**\n",
    "**We do fit_transform(X_train) and do only `transform(X_test)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Underfitting and Overfitting  \n",
    "Note: Squiggly line that passes through each and every point in the dataset.  \n",
    "`fit`- fit method tries to learn something about data.  \n",
    "`Transform`- Transform method tries to transform/normalize the data.  \n",
    "**Before Overfitting and Underfitting we need to learn about Bias and Variance.**  \n",
    "In training set - we see `Bias`  \n",
    "In testing set - we see `Varience`  \n",
    "The inablity of a model to capture the \"true\" relationship is called `Bias.`  \n",
    "**Underfitting**: The model with high bas and high varience.  \n",
    "**Overfitting**: The model with low bias and high varience.\n",
    "  \n",
    "For example, the relation bewteen weight and Height is a Curve.  \n",
    "split the data into Test and Train.  \n",
    "### Underfitting     \n",
    "If we use a straight line to fit relationship b/w Height & Weight then the line may not fit through all the points. i.e., the distance from the line to train points is high mean `high Bias` and also the distance from the line to test points is high mean ` high Varience.`.   \n",
    "### Overfitting       \n",
    "If we use a squiggly line to fit relationship b/w Height & Weight then it fit through all the train points. i.e., the distance from the line to train points is low/zero means `low Bias` and also the distance from the line to test points is high means `high Varience`  \n",
    "But, when we try to predict  \n",
    "it may be not that much accurate due to the relationship caused by the line(straight/squiggly).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test-Split  \n",
    "Do we need to always train test split- **yes, always**.  \n",
    "**Before we do Train-test-split, we need to divide our data frame into Features and Targets**  \n",
    "*X = Features or variables*  \n",
    "X= df.drop(\"col_name(Targets)\", axis=1)  \n",
    "*y = Targets or outcomes.*  \n",
    "y = df.col_name(Targets)/ df['col_name(Targets)']  \n",
    "**Use SKlearn to perform train test split**  \n",
    "from sklear.model_selection import train_test_split  \n",
    "[train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)  \n",
    "*Dont change the variable names order and always need to follow the same order.*  \n",
    "**X_train, X_test, y_train, y_test =**  \n",
    "The final formula is  \n",
    "**X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)**  \n",
    "test_size is 0.2/0.3 depends on data size, default=0.25.  \n",
    "random_state should be of any number.  \n",
    "**stratify**: This option is used when there is a large difference of distrubution of values(Probability) between train test.  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)  \n",
    "stratify = should either y or a column in X.  \n",
    "**The above method is Hold-out method - i.e., splitting data by train & test without any validation.**  \n",
    "## K-Fold Cross Validation.  \n",
    "In this we split the data for `train -- validate -- test`  \n",
    "*from sklearn.model_selection import KFold*\n",
    "*kf = KFold(n_splits=2, shuffle=True)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features, Target and OneHot Encoding.\n",
    "\n",
    "X- Features (i.e., model that uses parameters to learn.)  \n",
    "y- Targets (i.e., our model needs to predict the value.)  \n",
    "Features - what our model needs to learn form.  \n",
    "Target - What our model needs to predict.  \n",
    "### One-Hot Encoding.  \n",
    "Our model/system can not read the categories/names in the dataframe.  \n",
    "For this reason we use onehotEncoding to change categories/names into 0&1's.  \n",
    "Use pandas to do OneHotEncoding  \n",
    "**pd.get_dummies(df.Col_name, prefix=\"data\")**  \n",
    "col_name = column which consists of categroical strings in the df.  \n",
    "prefix = The name that adds before each element in the column.  \n",
    "*Some times we need to add our dummies to the dataframe to create a new dataframe which replaces categorical values with the numerical values.*  \n",
    "pd.concat([dummies,other columns df], axis=1)  \n",
    "axis =1 is very important. because it add our dummies to the columns."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
